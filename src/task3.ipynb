{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "instrument_route = '/Users/nathanjones/Downloads/NUWE/Hackathons/Schneider_DataScience/hackathon-schneider-pollution/data/raw/instrument_data.csv'\n",
    "\n",
    "df = pd.read_csv(instrument_route, parse_dates=[\"Measurement date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_ids = [205, 209, 223, 224, 226, 227]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Measurement date'] = df['Measurement date'].dt.floor('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3703662 entries, 0 to 3703661\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Dtype         \n",
      "---  ------             -----         \n",
      " 0   Measurement date   datetime64[ns]\n",
      " 1   Station code       int64         \n",
      " 2   Item code          int64         \n",
      " 3   Average value      float64       \n",
      " 4   Instrument status  int64         \n",
      "dtypes: datetime64[ns](1), float64(1), int64(3)\n",
      "memory usage: 141.3 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>feature engineering</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df, is_prediction=False):\n",
    "\n",
    "    items = {0: \"SO2\", 2: \"NO2\", 4: \"CO\", 5: \"O3\", 7: \"PM10\", 8: \"PM2.5\"}\n",
    "    # Pollutant column\n",
    "    df['Pollutant'] = df['Item code'].map(items)\n",
    "    \n",
    "    # Time-based features\n",
    "    df[\"Hour_sin\"] = np.sin(2 * np.pi * df[\"Measurement date\"].dt.hour / 24)\n",
    "    df[\"Hour_cos\"] = np.cos(2 * np.pi * df[\"Measurement date\"].dt.hour / 24)\n",
    "\n",
    "    df[\"Day_sin\"] = np.sin(2 * np.pi * df[\"Measurement date\"].dt.dayofweek / 7)\n",
    "    df[\"Day_cos\"] = np.cos(2 * np.pi * df[\"Measurement date\"].dt.dayofweek / 7)\n",
    "\n",
    "    df[\"Month_sin\"] = np.sin(2 * np.pi * df[\"Measurement date\"].dt.month / 12)\n",
    "    df[\"Month_cos\"] = np.cos(2 * np.pi * df[\"Measurement date\"].dt.month / 12)\n",
    "\n",
    "    # Sort the dataframe by date to calculate rolling average\n",
    "    df = df.sort_values(by=[\"Station code\", \"Item code\", \"Measurement date\"])\n",
    "\n",
    "    # Set 'Measurement date' as the index temporarily\n",
    "    df.set_index(\"Measurement date\", inplace=True)\n",
    "\n",
    "    # Apply rolling averages within each 'Station code' and 'Item code' group (6-month rolling window)\n",
    "    df[\"6_Month_Avg\"] = (\n",
    "        df.groupby([\"Station code\", \"Item code\"])[\"Average value\"]\n",
    "        .rolling(window=\"180d\", min_periods=1)  # 180 days = 6 months\n",
    "        .mean().round(3)\n",
    "        .reset_index(level=[0, 1], drop=True)\n",
    "    )\n",
    "\n",
    "    # Reset the index to restore 'Measurement date' as a column\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    # If this is for prediction, we should remove rows where the 6-month average is not available\n",
    "    if is_prediction:\n",
    "        df = df[df[\"6_Month_Avg\"].notna()]\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[704360  16980]\n",
      " [  6872  12521]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98    721340\n",
      "           1       0.42      0.65      0.51     19393\n",
      "\n",
      "    accuracy                           0.97    740733\n",
      "   macro avg       0.71      0.81      0.75    740733\n",
      "weighted avg       0.98      0.97      0.97    740733\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Copy entire DataFrame to handle Instrument status (binary)\n",
    "binary_df = df.copy()  \n",
    "binary_df['Instrument status'] = np.where(binary_df['Instrument status'] == 0, 0, 1)\n",
    "\n",
    "# Define features and target\n",
    "X = binary_df.drop(columns=['Instrument status'])  # Keep Measurement date and Pollutant for feature engineering\n",
    "y = binary_df['Instrument status']\n",
    "\n",
    "# Split data into train and test first to avoid data leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Now apply feature engineering separately for training and testing sets\n",
    "# Apply to training data\n",
    "train_data = pd.concat([X_train, y_train], axis=1)  # Concatenate to keep the target\n",
    "train_data = feature_engineering(train_data, is_prediction=False)\n",
    "\n",
    "# Apply to testing data\n",
    "test_data = pd.concat([X_test, y_test], axis=1)  # Concatenate to keep the target\n",
    "test_data = feature_engineering(test_data, is_prediction=True)\n",
    "\n",
    "# Separate features and target again after feature engineering\n",
    "X_train = train_data.drop(columns=['Instrument status', 'Measurement date', 'Pollutant'])\n",
    "y_train = train_data['Instrument status']\n",
    "X_test = test_data.drop(columns=['Instrument status', 'Measurement date', 'Pollutant'])\n",
    "y_test = test_data['Instrument status']\n",
    "\n",
    "# Handle imbalance using class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "# Train Random Forest Model\n",
    "rf_model = RandomForestClassifier(n_estimators=50, \n",
    "                                  class_weight=class_weight_dict,\n",
    "                                  max_depth=10, \n",
    "                                  min_samples_split=5,\n",
    "                                  random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate Model\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instrument status\n",
      "8    28323\n",
      "1    27147\n",
      "9    19668\n",
      "4    17960\n",
      "2     3868\n",
      "Name: count, dtype: int64\n",
      "[[4714    2   53  477  183]\n",
      " [ 170  214   27  142  221]\n",
      " [ 100   12 3358   96   26]\n",
      " [ 564   21   19 4878  183]\n",
      " [ 653   17   49  134 3081]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.87      0.81      5429\n",
      "           2       0.80      0.28      0.41       774\n",
      "           4       0.96      0.93      0.95      3592\n",
      "           8       0.85      0.86      0.86      5665\n",
      "           9       0.83      0.78      0.81      3934\n",
      "\n",
      "    accuracy                           0.84     19394\n",
      "   macro avg       0.84      0.74      0.77     19394\n",
      "weighted avg       0.84      0.84      0.83     19394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter out only faulty cases (Instrument status != 0)\n",
    "faulty_df = df[df['Instrument status'] != 0].copy()\n",
    "\n",
    "# Define features and target for faulty data\n",
    "X_faulty = faulty_df.drop(columns=['Instrument status'])  # Keep Measurement date and Pollutant for feature engineering\n",
    "y_faulty = faulty_df['Instrument status']  # Multi-class target\n",
    "\n",
    "print(y_faulty.value_counts())\n",
    "\n",
    "# Split data into train and test first to avoid data leakage\n",
    "X_train_faulty, X_test_faulty, y_train_faulty, y_test_faulty = train_test_split(\n",
    "    X_faulty, y_faulty, test_size=0.2, stratify=y_faulty, random_state=42\n",
    ")\n",
    "\n",
    "# Now apply feature engineering separately for training and testing sets\n",
    "# Apply to training data\n",
    "train_faulty_data = pd.concat([X_train_faulty, y_train_faulty], axis=1)  # Concatenate to keep the target\n",
    "train_faulty_data = feature_engineering(train_faulty_data, is_prediction=False)\n",
    "\n",
    "# Apply to testing data\n",
    "test_faulty_data = pd.concat([X_test_faulty, y_test_faulty], axis=1)  # Concatenate to keep the target\n",
    "test_faulty_data = feature_engineering(test_faulty_data, is_prediction=True)\n",
    "\n",
    "# Separate features and target again after feature engineering\n",
    "X_train_faulty = train_faulty_data.drop(columns=['Instrument status', 'Measurement date', 'Pollutant'])\n",
    "y_train_faulty = train_faulty_data['Instrument status']\n",
    "X_test_faulty = test_faulty_data.drop(columns=['Instrument status', 'Measurement date', 'Pollutant'])\n",
    "y_test_faulty = test_faulty_data['Instrument status']\n",
    "\n",
    "\n",
    "\n",
    "# Train Multi-Class Random Forest Model\n",
    "rf_multi = RandomForestClassifier(n_estimators=30, \n",
    "                                  max_depth=10, \n",
    "                                  min_samples_split=5,\n",
    "                                  random_state=42)\n",
    "rf_multi.fit(X_train_faulty, y_train_faulty)\n",
    "\n",
    "# Predictions\n",
    "y_pred_faulty = rf_multi.predict(X_test_faulty)\n",
    "\n",
    "# Evaluate Model\n",
    "print(confusion_matrix(y_test_faulty, y_pred_faulty))\n",
    "print(classification_report(y_test_faulty, y_pred_faulty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/nathanjones/Downloads/NUWE/Hackathons/Schneider_DataScience/hackathon-schneider-pollution/models/task_3/rf_multi_task_3.pkl']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(rf_model, f\"/Users/nathanjones/Downloads/NUWE/Hackathons/Schneider_DataScience/hackathon-schneider-pollution/models/task_3/rf_model_task_3.pkl\")\n",
    "joblib.dump(rf_multi, f\"/Users/nathanjones/Downloads/NUWE/Hackathons/Schneider_DataScience/hackathon-schneider-pollution/models/task_3/rf_multi_task_3.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>new dataframe for prediction</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the periods and station codes\n",
    "query_dict = {\n",
    "    205: {'Pollutant': 'SO2', 'start': '2023-11-01 00:00:00', 'end': '2023-11-30 23:00:00'},\n",
    "    209: {'Pollutant': 'NO2', 'start': '2023-09-01 00:00:00', 'end': '2023-09-30 23:00:00'},\n",
    "    223: {'Pollutant': 'O3', 'start': '2023-07-01 00:00:00', 'end': '2023-07-31 23:00:00'},\n",
    "    224: {'Pollutant': 'CO', 'start': '2023-10-01 00:00:00', 'end': '2023-10-31 23:00:00'},\n",
    "    226: {'Pollutant': 'PM10', 'start': '2023-08-01 00:00:00', 'end': '2023-08-31 23:00:00'},\n",
    "    227: {'Pollutant': 'PM2.5', 'start': '2023-12-01 00:00:00', 'end': '2023-12-31 23:00:00'}\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a DataFrame from the query_dict for the given periods and station codes\n",
    "new_data = []\n",
    "\n",
    "for station_code, query in query_dict.items():\n",
    "    # Generate a date range for the period\n",
    "    date_range = pd.date_range(start=query['start'], end=query['end'], freq='h')\n",
    "    \n",
    "    # Generate rows of data for each station code and pollutant\n",
    "    for date in date_range:\n",
    "        new_data.append({\n",
    "            'Station code': station_code,\n",
    "            'Pollutant': query['Pollutant'],\n",
    "            'Measurement date': date,\n",
    "            'Average value': np.nan  # Placeholder for the actual measurements\n",
    "        })\n",
    "\n",
    "# Convert new_data to a DataFrame\n",
    "new_df = pd.DataFrame(new_data)\n",
    "\n",
    "# Step 2: Concatenate the new data with your original training data (df)\n",
    "combined_df = pd.concat([df, new_df], ignore_index=True)\n",
    "combined_df_1 = pd.concat([df, new_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_pred(df, is_prediction=False):\n",
    "    items = {0: \"SO2\", 2: \"NO2\", 4: \"CO\", 5: \"O3\", 7: \"PM10\", 8: \"PM2.5\"}\n",
    "\n",
    "    # Map the Item code to Pollutant\n",
    "    df['Pollutant'] = df['Item code'].map(items)\n",
    "\n",
    "    # Extract time-based features (sin/cos for cyclic features like hour, day, month)\n",
    "    df[\"Hour_sin\"] = np.sin(2 * np.pi * df[\"Measurement date\"].dt.hour / 24)\n",
    "    df[\"Hour_cos\"] = np.cos(2 * np.pi * df[\"Measurement date\"].dt.hour / 24)\n",
    "    df[\"Day_sin\"] = np.sin(2 * np.pi * df[\"Measurement date\"].dt.dayofweek / 7)\n",
    "    df[\"Day_cos\"] = np.cos(2 * np.pi * df[\"Measurement date\"].dt.dayofweek / 7)\n",
    "    df[\"Month_sin\"] = np.sin(2 * np.pi * df[\"Measurement date\"].dt.month / 12)\n",
    "    df[\"Month_cos\"] = np.cos(2 * np.pi * df[\"Measurement date\"].dt.month / 12)\n",
    "\n",
    "    # Ensure 'Measurement date' is in datetime format\n",
    "    df['Measurement date'] = pd.to_datetime(df['Measurement date'])\n",
    "\n",
    "    # Drop duplicate entries based on key columns for prediction periods\n",
    "    df = df.drop_duplicates(subset=[\"Measurement date\", \"Station code\", \"Item code\"])\n",
    "\n",
    "    # Create a composite index to ensure uniqueness (Measurement date + Station code + Item code)\n",
    "    df.set_index([\"Measurement date\", \"Station code\", \"Item code\"], inplace=True)\n",
    "\n",
    "    # Apply rolling averages (6 months for historical data, min_periods=1 for prediction)\n",
    "    if not is_prediction:\n",
    "        df = df.sort_values(by=[\"Station code\", \"Item code\", \"Measurement date\"])\n",
    "\n",
    "        # 6-month (180 days) rolling average\n",
    "        df[\"6_Month_Avg\"] = (\n",
    "            df.groupby([\"Station code\", \"Item code\"])[\"Average value\"]\n",
    "            .rolling(window=\"180d\", min_periods=1)  # 180 days = 6 months\n",
    "            .mean()\n",
    "            .reset_index(level=[0, 1], drop=True)\n",
    "        )\n",
    "\n",
    "    if is_prediction:\n",
    "        # For prediction, apply rolling averages even if no past data exists.\n",
    "        # Ensure we use a proper numeric window\n",
    "        df[\"6_Month_Avg\"] = (\n",
    "            df.groupby([\"Station code\", \"Item code\"])[\"Average value\"]\n",
    "            .rolling(window=180, min_periods=1)  # Use numeric window size (in hours or data points)\n",
    "            .mean()\n",
    "            .reset_index(level=[0, 1], drop=True)\n",
    "        )\n",
    "\n",
    "    # Reset the index to restore 'Measurement date' as a column\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Apply your feature engineering to the combined DataFrame\n",
    "# Assuming feature_engineering is the function you defined before\n",
    "combined_df = feature_engineering_pred(combined_df, is_prediction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Prepare the features for prediction (drop unnecessary columns)\n",
    "X_combined = combined_df[['Station code', 'Item code', 'Average value', 'Hour_sin',\n",
    "       'Hour_cos', 'Day_sin', 'Day_cos', 'Month_sin', 'Month_cos',\n",
    "       '6_Month_Avg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Make binary predictions on the new data\n",
    "predictions = rf_multi.predict(X_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Add binary predictions to the DataFrame\n",
    "combined_df_1['Predicted Instrument status'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Measurement date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "Station code",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Item code",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Average value",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Instrument status",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Pollutant",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Predicted Instrument status",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "f3796089-182f-4329-9324-346761c58e9a",
       "rows": [
        [
         "155430",
         "2021-01-01 00:00:00",
         "205",
         "0.0",
         "0.006",
         "0.0",
         null,
         "9"
        ],
        [
         "155431",
         "2021-01-01 00:00:00",
         "205",
         "2.0",
         "0.068",
         "0.0",
         null,
         "9"
        ],
        [
         "155432",
         "2021-01-01 00:00:00",
         "205",
         "4.0",
         "1.3",
         "0.0",
         null,
         "9"
        ],
        [
         "155433",
         "2021-01-01 00:00:00",
         "205",
         "5.0",
         "0.002",
         "0.0",
         null,
         "9"
        ],
        [
         "155434",
         "2021-01-01 00:00:00",
         "205",
         "7.0",
         "77.0",
         "0.0",
         null,
         "9"
        ],
        [
         "155435",
         "2021-01-01 00:00:00",
         "205",
         "8.0",
         "63.0",
         "0.0",
         null,
         "9"
        ],
        [
         "155436",
         "2021-01-01 01:00:00",
         "205",
         "0.0",
         "0.006",
         "0.0",
         null,
         "9"
        ],
        [
         "155437",
         "2021-01-01 01:00:00",
         "205",
         "2.0",
         "0.066",
         "0.0",
         null,
         "9"
        ],
        [
         "155438",
         "2021-01-01 01:00:00",
         "205",
         "4.0",
         "1.4",
         "0.0",
         null,
         "9"
        ],
        [
         "155439",
         "2021-01-01 01:00:00",
         "205",
         "5.0",
         "0.002",
         "0.0",
         null,
         "9"
        ],
        [
         "155440",
         "2021-01-01 01:00:00",
         "205",
         "7.0",
         "76.0",
         "0.0",
         null,
         "9"
        ],
        [
         "155441",
         "2021-01-01 01:00:00",
         "205",
         "8.0",
         "63.0",
         "0.0",
         null,
         "9"
        ],
        [
         "155442",
         "2021-01-01 02:00:00",
         "205",
         "0.0",
         "0.005",
         "0.0",
         null,
         "9"
        ],
        [
         "155443",
         "2021-01-01 02:00:00",
         "205",
         "2.0",
         "0.063",
         "0.0",
         null,
         "9"
        ],
        [
         "155444",
         "2021-01-01 02:00:00",
         "205",
         "4.0",
         "1.2",
         "0.0",
         null,
         "9"
        ],
        [
         "155445",
         "2021-01-01 02:00:00",
         "205",
         "5.0",
         "0.002",
         "0.0",
         null,
         "9"
        ],
        [
         "155446",
         "2021-01-01 02:00:00",
         "205",
         "7.0",
         "73.0",
         "0.0",
         null,
         "9"
        ],
        [
         "155447",
         "2021-01-01 02:00:00",
         "205",
         "8.0",
         "57.0",
         "0.0",
         null,
         "9"
        ],
        [
         "155448",
         "2021-01-01 03:00:00",
         "205",
         "0.0",
         "0.005",
         "0.0",
         null,
         "9"
        ],
        [
         "155449",
         "2021-01-01 03:00:00",
         "205",
         "2.0",
         "0.053",
         "0.0",
         null,
         "9"
        ],
        [
         "155450",
         "2021-01-01 03:00:00",
         "205",
         "4.0",
         "1.1",
         "0.0",
         null,
         "9"
        ],
        [
         "155451",
         "2021-01-01 03:00:00",
         "205",
         "5.0",
         "0.002",
         "0.0",
         null,
         "9"
        ],
        [
         "155452",
         "2021-01-01 03:00:00",
         "205",
         "7.0",
         "67.0",
         "0.0",
         null,
         "9"
        ],
        [
         "155453",
         "2021-01-01 03:00:00",
         "205",
         "8.0",
         "55.0",
         "0.0",
         null,
         "9"
        ],
        [
         "155454",
         "2021-01-01 04:00:00",
         "205",
         "0.0",
         "0.004",
         "0.0",
         null,
         "9"
        ],
        [
         "155455",
         "2021-01-01 04:00:00",
         "205",
         "2.0",
         "0.051",
         "0.0",
         null,
         "9"
        ],
        [
         "155456",
         "2021-01-01 04:00:00",
         "205",
         "4.0",
         "1.1",
         "0.0",
         null,
         "9"
        ],
        [
         "155457",
         "2021-01-01 04:00:00",
         "205",
         "5.0",
         "0.002",
         "0.0",
         null,
         "9"
        ],
        [
         "155458",
         "2021-01-01 04:00:00",
         "205",
         "7.0",
         "66.0",
         "0.0",
         null,
         "9"
        ],
        [
         "155459",
         "2021-01-01 04:00:00",
         "205",
         "8.0",
         "54.0",
         "0.0",
         null,
         "9"
        ],
        [
         "155460",
         "2021-01-01 05:00:00",
         "205",
         "0.0",
         "0.004",
         "0.0",
         null,
         "9"
        ],
        [
         "155461",
         "2021-01-01 05:00:00",
         "205",
         "2.0",
         "0.05",
         "0.0",
         null,
         "9"
        ],
        [
         "155462",
         "2021-01-01 05:00:00",
         "205",
         "4.0",
         "1.1",
         "0.0",
         null,
         "9"
        ],
        [
         "155463",
         "2021-01-01 05:00:00",
         "205",
         "5.0",
         "0.002",
         "0.0",
         null,
         "9"
        ],
        [
         "155464",
         "2021-01-01 05:00:00",
         "205",
         "7.0",
         "66.0",
         "0.0",
         null,
         "9"
        ],
        [
         "155465",
         "2021-01-01 05:00:00",
         "205",
         "8.0",
         "55.0",
         "0.0",
         null,
         "9"
        ],
        [
         "155466",
         "2021-01-01 06:00:00",
         "205",
         "0.0",
         "0.005",
         "0.0",
         null,
         "9"
        ],
        [
         "155467",
         "2021-01-01 06:00:00",
         "205",
         "2.0",
         "0.05",
         "0.0",
         null,
         "9"
        ],
        [
         "155468",
         "2021-01-01 06:00:00",
         "205",
         "4.0",
         "1.1",
         "0.0",
         null,
         "9"
        ],
        [
         "155469",
         "2021-01-01 06:00:00",
         "205",
         "5.0",
         "0.002",
         "0.0",
         null,
         "9"
        ],
        [
         "155470",
         "2021-01-01 06:00:00",
         "205",
         "7.0",
         "70.0",
         "0.0",
         null,
         "9"
        ],
        [
         "155471",
         "2021-01-01 06:00:00",
         "205",
         "8.0",
         "56.0",
         "0.0",
         null,
         "9"
        ],
        [
         "155472",
         "2021-01-01 07:00:00",
         "205",
         "0.0",
         "0.004",
         "0.0",
         null,
         "9"
        ],
        [
         "155473",
         "2021-01-01 07:00:00",
         "205",
         "2.0",
         "0.05",
         "0.0",
         null,
         "8"
        ],
        [
         "155474",
         "2021-01-01 07:00:00",
         "205",
         "4.0",
         "1.0",
         "0.0",
         null,
         "9"
        ],
        [
         "155475",
         "2021-01-01 07:00:00",
         "205",
         "5.0",
         "0.002",
         "0.0",
         null,
         "9"
        ],
        [
         "155476",
         "2021-01-01 07:00:00",
         "205",
         "7.0",
         "69.0",
         "0.0",
         null,
         "9"
        ],
        [
         "155477",
         "2021-01-01 07:00:00",
         "205",
         "8.0",
         "58.0",
         "0.0",
         null,
         "9"
        ],
        [
         "155478",
         "2021-01-01 08:00:00",
         "205",
         "0.0",
         "0.005",
         "0.0",
         null,
         "9"
        ],
        [
         "155479",
         "2021-01-01 08:00:00",
         "205",
         "2.0",
         "0.046",
         "0.0",
         null,
         "8"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 147438
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Measurement date</th>\n",
       "      <th>Station code</th>\n",
       "      <th>Item code</th>\n",
       "      <th>Average value</th>\n",
       "      <th>Instrument status</th>\n",
       "      <th>Pollutant</th>\n",
       "      <th>Predicted Instrument status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>155430</th>\n",
       "      <td>2021-01-01 00:00:00</td>\n",
       "      <td>205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155431</th>\n",
       "      <td>2021-01-01 00:00:00</td>\n",
       "      <td>205</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155432</th>\n",
       "      <td>2021-01-01 00:00:00</td>\n",
       "      <td>205</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155433</th>\n",
       "      <td>2021-01-01 00:00:00</td>\n",
       "      <td>205</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155434</th>\n",
       "      <td>2021-01-01 00:00:00</td>\n",
       "      <td>205</td>\n",
       "      <td>7.0</td>\n",
       "      <td>77.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3704377</th>\n",
       "      <td>2023-11-30 19:00:00</td>\n",
       "      <td>205</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SO2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3704378</th>\n",
       "      <td>2023-11-30 20:00:00</td>\n",
       "      <td>205</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SO2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3704379</th>\n",
       "      <td>2023-11-30 21:00:00</td>\n",
       "      <td>205</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SO2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3704380</th>\n",
       "      <td>2023-11-30 22:00:00</td>\n",
       "      <td>205</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SO2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3704381</th>\n",
       "      <td>2023-11-30 23:00:00</td>\n",
       "      <td>205</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SO2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147438 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Measurement date  Station code  Item code  Average value  \\\n",
       "155430  2021-01-01 00:00:00           205        0.0          0.006   \n",
       "155431  2021-01-01 00:00:00           205        2.0          0.068   \n",
       "155432  2021-01-01 00:00:00           205        4.0          1.300   \n",
       "155433  2021-01-01 00:00:00           205        5.0          0.002   \n",
       "155434  2021-01-01 00:00:00           205        7.0         77.000   \n",
       "...                     ...           ...        ...            ...   \n",
       "3704377 2023-11-30 19:00:00           205        NaN            NaN   \n",
       "3704378 2023-11-30 20:00:00           205        NaN            NaN   \n",
       "3704379 2023-11-30 21:00:00           205        NaN            NaN   \n",
       "3704380 2023-11-30 22:00:00           205        NaN            NaN   \n",
       "3704381 2023-11-30 23:00:00           205        NaN            NaN   \n",
       "\n",
       "         Instrument status Pollutant  Predicted Instrument status  \n",
       "155430                 0.0       NaN                            9  \n",
       "155431                 0.0       NaN                            9  \n",
       "155432                 0.0       NaN                            9  \n",
       "155433                 0.0       NaN                            9  \n",
       "155434                 0.0       NaN                            9  \n",
       "...                    ...       ...                          ...  \n",
       "3704377                NaN       SO2                            8  \n",
       "3704378                NaN       SO2                            8  \n",
       "3704379                NaN       SO2                            8  \n",
       "3704380                NaN       SO2                            8  \n",
       "3704381                NaN       SO2                            8  \n",
       "\n",
       "[147438 rows x 7 columns]"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check results exist in given period\n",
    "combined_df_1.loc[combined_df[\"Station code\"]==205]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 7 \n",
    "# To store results in JSON format\n",
    "target = {}\n",
    "\n",
    "# Loop through the query_dict and filter the dataframe accordingly\n",
    "for station_code, query in query_dict.items():\n",
    "    # Filter the dataframe by Station code, Pollutant, and Date range\n",
    "    filtered_df = combined_df_1[\n",
    "        (combined_df_1['Station code'] == station_code) &\n",
    "        (combined_df_1['Pollutant'] == query['Pollutant']) &\n",
    "        (combined_df_1['Measurement date'] >= query['start']) &\n",
    "        (combined_df_1['Measurement date'] <= query['end'])\n",
    "    ]\n",
    "    \n",
    "    # Select only the columns needed for predictions\n",
    "    prediction_columns = ['Measurement date', 'Station code', 'Pollutant', 'Predicted Instrument status']\n",
    "    \n",
    "    filtered_df = filtered_df[prediction_columns]\n",
    "    \n",
    "    # Convert to dictionary format for JSON\n",
    "    target[station_code] = filtered_df.set_index('Measurement date').to_dict(orient='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert to the required JSON format\n",
    "formatted_target = {\n",
    "    \"target\": {\n",
    "        station_code: {\n",
    "            timestamp.strftime(\"%Y-%m-%d %H:%M:%S\"): data[\"Predicted Instrument status\"]\n",
    "            for timestamp, data in records.items()\n",
    "        }\n",
    "        for station_code, records in target.items()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convert to JSON string\n",
    "json_output = json.dumps(formatted_target, indent=4)\n",
    "\n",
    "#with open(\"/Users/nathanjones/Downloads/NUWE/Hackathons/Schneider_DataScience/hackathon-schneider-pollution/predictions/predictions_task_3.json\", \"w\") as f:\n",
    "   # json.dump(formatted_target, f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
